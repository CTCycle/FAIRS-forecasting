{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# set warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category = Warning)\n",
    "\n",
    "# add parent folder path to the namespace\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# import modules and components\n",
    "from utils.model_assets import Inference, ModelValidation\n",
    "import utils.global_paths as globpt\n",
    "import configurations as cnf\n",
    "\n",
    "# specify relative paths from global paths and create subfolders\n",
    "cp_path = os.path.join(globpt.train_path, 'checkpoints')\n",
    "pred_path = os.path.join(globpt.inference_path, 'predictions')\n",
    "os.mkdir(cp_path) if not os.path.exists(cp_path) else None\n",
    "os.mkdir(pred_path) if not os.path.exists(pred_path) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "inference = Inference(cnf.seed) \n",
    "model, parameters = inference.load_pretrained_model(cp_path)\n",
    "model_folder = inference.folder_path\n",
    "model.summary(expand_nested=True)\n",
    "\n",
    "# Load normalizer and encoders\n",
    "pp_path = os.path.join(model_folder, 'preprocessing')\n",
    "if parameters['model_name']=='CCM':    \n",
    "    encoder_path = os.path.join(pp_path, 'categorical_encoder.pkl')\n",
    "    with open(encoder_path, 'rb') as file:\n",
    "        encoder = pickle.load(file)    \n",
    "\n",
    "# load npy files\n",
    "if parameters['model_name']=='CCM':\n",
    "    pp_path = os.path.join(model_folder, 'preprocessing')\n",
    "    X_train = np.load(os.path.join(pp_path, 'train_data.npy'))\n",
    "    Y_train_OHE = np.load(os.path.join(pp_path, 'train_labels.npy'))\n",
    "    X_test = np.load(os.path.join(pp_path, 'test_data.npy'))\n",
    "    Y_test_OHE = np.load(os.path.join(pp_path, 'test_labels.npy'))\n",
    "    train_inputs, train_outputs = X_train, Y_train_OHE\n",
    "    test_inputs, test_outputs = X_test, Y_test_OHE\n",
    "elif parameters['model_name']=='NMM':\n",
    "    pp_path = os.path.join(model_folder, 'preprocessing')\n",
    "    X_train_ext = np.load(os.path.join(pp_path, 'train_extractions.npy'))\n",
    "    X_train_pos = np.load(os.path.join(pp_path, 'train_positions.npy'))\n",
    "    Y_train_OHE = np.load(os.path.join(pp_path, 'train_labels.npy'))\n",
    "    X_test_ext = np.load(os.path.join(pp_path, 'test_extractions.npy'))\n",
    "    X_test_pos = np.load(os.path.join(pp_path, 'test_positions.npy'))\n",
    "    Y_test_OHE = np.load(os.path.join(pp_path, 'test_labels.npy'))\n",
    "    train_inputs, train_outputs = [X_train_ext, X_train_pos], Y_train_OHE\n",
    "    test_inputs, test_outputs = [X_test_ext, X_test_pos], Y_test_OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluation of loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = ModelValidation(model)\n",
    "\n",
    "# create subfolder for evaluation data\n",
    "eval_path = os.path.join(model_folder, 'evaluation') \n",
    "os.mkdir(eval_path) if not os.path.exists(eval_path) else None\n",
    "\n",
    "# predict lables from train set\n",
    "predicted_train = model.predict(train_inputs, verbose=0)\n",
    "y_pred = np.argmax(predicted_train, axis=-1)\n",
    "y_true = np.argmax(train_outputs, axis=-1)\n",
    "\n",
    "# show predicted classes (train dataset)\n",
    "class_pred, class_true = np.unique(y_pred), np.unique(y_true)\n",
    "print(f'''\n",
    "Number of classes observed in train (true labels): {len(class_true)}\n",
    "Number of classes observed in train (predicted labels): {len(class_pred)}\n",
    "-------------------------------------------------------------------------------\n",
    "Classes observed in predicted train labels:\n",
    "-------------------------------------------------------------------------------\n",
    "{class_pred}\n",
    "''')\n",
    "\n",
    "# predict labels from test set\n",
    "predicted_test = model.predict(test_inputs, verbose=0)\n",
    "y_pred_labels = np.argmax(predicted_test, axis=-1)\n",
    "y_true_labels = np.argmax(test_outputs, axis=-1)\n",
    "\n",
    "# show predicted classes (testdataset)\n",
    "class_pred, class_true = np.unique(y_pred), np.unique(y_true)\n",
    "print(f'''\n",
    "Number of classes observed in test (true labels): {len(class_true)}\n",
    "Number of classes observed in test (predicted labels): {len(class_pred)}\n",
    "-------------------------------------------------------------------------------\n",
    "Classes observed in predicted test labels:\n",
    "-------------------------------------------------------------------------------\n",
    "{class_pred}\n",
    "-------------------------------------------------------------------------------\n",
    "''')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate confusion matrix from train set (if class num is equal)\n",
    "try:\n",
    "    validator.FAIRS_confusion(y_true, y_pred, 'confusion_matrix_train', eval_path)    \n",
    "except Exception as e:\n",
    "    print('Could not generate confusion matrix for train dataset')\n",
    "    print('Error:', str(e))\n",
    "\n",
    "# generate confusion matrix from test set (if class num is equal)\n",
    "try:\n",
    "    validator.FAIRS_confusion(y_true, y_pred, 'confusion_matrix_test', eval_path)        \n",
    "except Exception as e:\n",
    "    print('Could not generate confusion matrix for test dataset')\n",
    "    print('Error:', str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aquarius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
