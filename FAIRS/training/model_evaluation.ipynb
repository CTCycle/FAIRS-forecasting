{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# set warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category = Warning)\n",
    "\n",
    "# add parent folder path to the namespace\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# import modules and components\n",
    "from utils.inference import Inference\n",
    "from utils.validation import BaselineModels, ModelValidation\n",
    "import utils.global_paths as globpt\n",
    "import configurations as cnf\n",
    "\n",
    "# specify relative paths from global paths and create subfolders\n",
    "cp_path = os.path.join(globpt.train_path, 'checkpoints')\n",
    "res_path = os.path.join(globpt.data_path, 'analysis')\n",
    "os.mkdir(cp_path) if not os.path.exists(cp_path) else None\n",
    "os.mkdir(res_path) if not os.path.exists(res_path) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and model\n",
    "\n",
    "Load the pretrained model from training/checkpoints. Preprocessed data is loaded and use to benchmark baseline model for their accuracy in predicting next roulette extractions. The data is extracted according to the selected type of pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "inference = Inference(cnf.seed) \n",
    "model, parameters = inference.load_pretrained_model(cp_path)\n",
    "model_folder = inference.folder_path\n",
    "model.summary(expand_nested=True)\n",
    "\n",
    "# Load normalizer and encoders\n",
    "pp_path = os.path.join(model_folder, 'preprocessing')\n",
    "if parameters['model_name']=='CCM':    \n",
    "    encoder_path = os.path.join(pp_path, 'categorical_encoder.pkl')\n",
    "    with open(encoder_path, 'rb') as file:\n",
    "        encoder = pickle.load(file)    \n",
    "\n",
    "# load npy files\n",
    "if parameters['model_name']=='CCM':\n",
    "    pp_path = os.path.join(model_folder, 'preprocessing')\n",
    "    X_train = np.load(os.path.join(pp_path, 'train_data.npy'))\n",
    "    Y_train_OHE = np.load(os.path.join(pp_path, 'train_labels.npy'))\n",
    "    X_test = np.load(os.path.join(pp_path, 'test_data.npy'))\n",
    "    Y_test_OHE = np.load(os.path.join(pp_path, 'test_labels.npy'))\n",
    "    train_inputs, train_outputs = X_train, Y_train_OHE\n",
    "    test_inputs, test_outputs = X_test, Y_test_OHE\n",
    "elif parameters['model_name']=='NMM':\n",
    "    pp_path = os.path.join(model_folder, 'preprocessing')\n",
    "    X_train_ext = np.load(os.path.join(pp_path, 'train_extractions.npy'))\n",
    "    X_train_pos = np.load(os.path.join(pp_path, 'train_positions.npy'))\n",
    "    Y_train_OHE = np.load(os.path.join(pp_path, 'train_labels.npy'))\n",
    "    X_test_ext = np.load(os.path.join(pp_path, 'test_extractions.npy'))\n",
    "    X_test_pos = np.load(os.path.join(pp_path, 'test_positions.npy'))\n",
    "    Y_test_OHE = np.load(os.path.join(pp_path, 'test_labels.npy'))\n",
    "    train_inputs, train_outputs = [X_train_ext, X_train_pos], Y_train_OHE\n",
    "    test_inputs, test_outputs = [X_test_ext, X_test_pos], Y_test_OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = BaselineModels()\n",
    "if parameters['model_name']=='NMM':\n",
    "    X_train = np.concatenate((X_train_ext, X_train_pos), axis=1)\n",
    "    X_test = np.concatenate((X_test_ext, X_test_pos), axis=1)\n",
    "\n",
    "if parameters['inverted_test'] == False:\n",
    "    train_idx = [x for x in range(len(X_train))]\n",
    "    test_idx = [x + len(train_idx) for x in range(len(X_test))]\n",
    "else:\n",
    "    test_idx = [x for x in range(len(X_test))]\n",
    "    train_idx = [x + len(test_idx) for x in range(len(X_train))]    \n",
    "\n",
    "train_data = (X_train.squeeze(), Y_train_OHE)\n",
    "test_data = (X_test.squeeze(), Y_test_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Decision Tree and Random Forest classifier\n",
    "\n",
    "Use decision tree and random forest classifiers on the train and test dataset. Calculate accuracy score to compare with the FAIRS deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = 100\n",
    "DT_model = baseline.DecisionTree_classifier(train_data, seed=cnf.seed)\n",
    "RFC_model = baseline.RandomForest_classifier(train_data, estimators=estimators, seed=cnf.seed)\n",
    "DT_train_acc, DT_test_acc = baseline.model_accuracy(DT_model, train_data, test_data)\n",
    "RFC_train_acc, RFC_test_acc = baseline.model_accuracy(RFC_model, train_data, test_data)\n",
    "\n",
    "# print accuracy report\n",
    "print('\\nDecision Tree baseline model')\n",
    "print(f'Train accuracy: {DT_train_acc}')\n",
    "print(f'Test accuracy: {DT_test_acc}')\n",
    "print(f'\\nRandom Forest baseline model (estimators = {estimators})')\n",
    "print(f'Train accuracy: {RFC_train_acc}')\n",
    "print(f'Test accuracy: {RFC_test_acc}')\n",
    "\n",
    "# generate predictions to compare them to real labels\n",
    "validator = ModelValidation()\n",
    "train_predictions = DT_model.predict(X_train.squeeze())\n",
    "test_predictions = DT_model.predict(X_test.squeeze())\n",
    "train_predictions = np.argmax(train_predictions, axis=1)\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "Y_train = np.argmax(Y_train_OHE, axis=1)\n",
    "Y_test = np.argmax(Y_test_OHE, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the true timeseries versus the predicted timeseries, and show the confusion matrix for both the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'train' : [train_idx, Y_train.tolist(), train_predictions.tolist()], \n",
    "          'test' : [test_idx, Y_test.tolist(), test_predictions.tolist()]}\n",
    "validator.plot_timeseries_prediction(values, 'decision_tree_predictions', res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 FAIRS pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = ModelValidation()\n",
    "\n",
    "# create subfolder for evaluation data\n",
    "eval_path = os.path.join(model_folder, 'evaluation') \n",
    "os.mkdir(eval_path) if not os.path.exists(eval_path) else None\n",
    "\n",
    "# predict lables from train set\n",
    "predicted_train = model.predict(train_inputs, verbose=0)\n",
    "y_pred = np.argmax(predicted_train, axis=-1)\n",
    "y_true = np.argmax(train_outputs, axis=-1)\n",
    "\n",
    "# show predicted classes (train dataset)\n",
    "class_pred, class_true = np.unique(y_pred), np.unique(y_true)\n",
    "print(f'\\nNumber of classes observed in train (true labels): {len(class_true)}')\n",
    "print(f'Number of classes observed in train (predicted labels): {len(class_pred)}')\n",
    "print(f'Classes observed in predicted train labels:\\n{class_pred}')\n",
    "\n",
    "# predict labels from test set\n",
    "predicted_test = model.predict(test_inputs, verbose=0)\n",
    "y_pred_labels = np.argmax(predicted_test, axis=-1)\n",
    "y_true_labels = np.argmax(test_outputs, axis=-1)\n",
    "\n",
    "# show predicted classes (test dataset)\n",
    "class_pred, class_true = np.unique(y_pred), np.unique(y_true)\n",
    "print(f'\\nNumber of classes observed in test (true labels): {len(class_true)}')\n",
    "print(f'Number of classes observed in test (predicted labels): {len(class_pred)}')\n",
    "print(f'Classes observed in predicted test labels:\\n{class_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate confusion matrix from train set (if class num is equal)\n",
    "validator.plot_confusion_matrix(y_true, y_pred, 'confusion_matrix_train', eval_path)    \n",
    "validator.plot_confusion_matrix(y_true, y_pred, 'confusion_matrix_test', eval_path)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aquarius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
